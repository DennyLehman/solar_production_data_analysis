{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/katherineli/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd # pandas for data manipulation / analysis\n",
    "import numpy as np # numpy for math\n",
    "import matplotlib.pyplot as plt # pyplot for plotting and visualization\n",
    "import datetime # datetime for computing times\n",
    "import seaborn as sns # fancier plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 8 # set figsize for all future plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holding_company_id</th>\n",
       "      <th>project_company_id</th>\n",
       "      <th>contract_id</th>\n",
       "      <th>size_kwdc</th>\n",
       "      <th>created_on</th>\n",
       "      <th>updated_on</th>\n",
       "      <th>production_date</th>\n",
       "      <th>ato_date</th>\n",
       "      <th>actual_kwh</th>\n",
       "      <th>expected_kwh</th>\n",
       "      <th>...</th>\n",
       "      <th>host_type</th>\n",
       "      <th>revenue_type</th>\n",
       "      <th>interconnection_type</th>\n",
       "      <th>registry_facility_name</th>\n",
       "      <th>cref_short_name</th>\n",
       "      <th>subscriber_orginization</th>\n",
       "      <th>system_yield_as_built</th>\n",
       "      <th>system_yield_finance</th>\n",
       "      <th>annual_degradation_engr</th>\n",
       "      <th>annual_degradation_finance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>44</td>\n",
       "      <td>4FbAzTVD</td>\n",
       "      <td>25.13</td>\n",
       "      <td>49:53.6</td>\n",
       "      <td>53:14.9</td>\n",
       "      <td>00:00.0</td>\n",
       "      <td>11/29/18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.908097</td>\n",
       "      <td>...</td>\n",
       "      <td>C&amp;I</td>\n",
       "      <td>NEM</td>\n",
       "      <td>NEM</td>\n",
       "      <td>NON241648</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1131.0</td>\n",
       "      <td>1131.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>50</td>\n",
       "      <td>56XBWDhT</td>\n",
       "      <td>40.00</td>\n",
       "      <td>49:53.6</td>\n",
       "      <td>53:14.9</td>\n",
       "      <td>00:00.0</td>\n",
       "      <td>9/28/18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.340590</td>\n",
       "      <td>...</td>\n",
       "      <td>C&amp;I</td>\n",
       "      <td>NEM</td>\n",
       "      <td>NEM</td>\n",
       "      <td>NON241695</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>954.7</td>\n",
       "      <td>1170.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>71</td>\n",
       "      <td>5Q6T8Cm7</td>\n",
       "      <td>6.70</td>\n",
       "      <td>49:53.6</td>\n",
       "      <td>53:14.9</td>\n",
       "      <td>00:00.0</td>\n",
       "      <td>11/28/18</td>\n",
       "      <td>0.139945</td>\n",
       "      <td>12.643548</td>\n",
       "      <td>...</td>\n",
       "      <td>Residential</td>\n",
       "      <td>NEM</td>\n",
       "      <td>NEM</td>\n",
       "      <td>NON241642</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>43</td>\n",
       "      <td>8EaZmYXK</td>\n",
       "      <td>26.18</td>\n",
       "      <td>49:53.6</td>\n",
       "      <td>53:14.9</td>\n",
       "      <td>00:00.0</td>\n",
       "      <td>3/10/18</td>\n",
       "      <td>23.113000</td>\n",
       "      <td>50.342822</td>\n",
       "      <td>...</td>\n",
       "      <td>C&amp;I</td>\n",
       "      <td>CREF</td>\n",
       "      <td>CREF</td>\n",
       "      <td>NON241367</td>\n",
       "      <td>NCS07</td>\n",
       "      <td>Arcadia</td>\n",
       "      <td>1240.8</td>\n",
       "      <td>1327.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>44</td>\n",
       "      <td>8GniqiNd</td>\n",
       "      <td>77.52</td>\n",
       "      <td>49:53.6</td>\n",
       "      <td>53:14.9</td>\n",
       "      <td>00:00.0</td>\n",
       "      <td>7/12/18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>142.074655</td>\n",
       "      <td>...</td>\n",
       "      <td>C&amp;I</td>\n",
       "      <td>NEM</td>\n",
       "      <td>NEM</td>\n",
       "      <td>NON241708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1214.0</td>\n",
       "      <td>1214.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   holding_company_id  project_company_id contract_id  size_kwdc created_on  \\\n",
       "0                  15                  44    4FbAzTVD      25.13    49:53.6   \n",
       "1                  14                  50    56XBWDhT      40.00    49:53.6   \n",
       "2                  28                  71    5Q6T8Cm7       6.70    49:53.6   \n",
       "3                  14                  43    8EaZmYXK      26.18    49:53.6   \n",
       "4                  15                  44    8GniqiNd      77.52    49:53.6   \n",
       "\n",
       "  updated_on production_date  ato_date  actual_kwh  expected_kwh  ...  \\\n",
       "0    53:14.9         00:00.0  11/29/18         NaN     42.908097  ...   \n",
       "1    53:14.9         00:00.0   9/28/18         NaN     71.340590  ...   \n",
       "2    53:14.9         00:00.0  11/28/18    0.139945     12.643548  ...   \n",
       "3    53:14.9         00:00.0   3/10/18   23.113000     50.342822  ...   \n",
       "4    53:14.9         00:00.0   7/12/18         NaN    142.074655  ...   \n",
       "\n",
       "     host_type  revenue_type interconnection_type registry_facility_name  \\\n",
       "0          C&I           NEM                  NEM              NON241648   \n",
       "1          C&I           NEM                  NEM              NON241695   \n",
       "2  Residential           NEM                  NEM              NON241642   \n",
       "3          C&I          CREF                 CREF              NON241367   \n",
       "4          C&I           NEM                  NEM              NON241708   \n",
       "\n",
       "  cref_short_name subscriber_orginization system_yield_as_built  \\\n",
       "0             NaN                     NaN                1131.0   \n",
       "1             NaN                     NaN                 954.7   \n",
       "2             NaN                     NaN                1250.0   \n",
       "3           NCS07                 Arcadia                1240.8   \n",
       "4             NaN                     NaN                1214.0   \n",
       "\n",
       "  system_yield_finance annual_degradation_engr  annual_degradation_finance  \n",
       "0               1131.0                   0.005                         NaN  \n",
       "1               1170.0                   0.005                       0.005  \n",
       "2               1250.0                   0.005                       0.005  \n",
       "3               1327.0                   0.005                         NaN  \n",
       "4               1214.0                   0.005                         NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "data = pd.read_csv('daily_contract_production.csv', header = None)\n",
    "data.columns = ['holding_company_id', 'project_company_id', 'contract_id', 'size_kwdc', 'created_on', \\\n",
    "                'updated_on', 'production_date', 'ato_date', 'actual_kwh', 'expected_kwh', \\\n",
    "                'weather_adjusted_expected_kwh']\n",
    "extra = pd.read_csv('d_contracts.csv')\n",
    "\n",
    "union = data.join(extra.set_index('id'), on = 'contract_id', rsuffix=\"DROP\").filter(regex=\"^(?!.*DROP)\")\n",
    "\n",
    "union.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 84216 entries, 0 to 84215\n",
      "Data columns (total 23 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   holding_company_id             84216 non-null  int64  \n",
      " 1   project_company_id             84216 non-null  int64  \n",
      " 2   contract_id                    84216 non-null  object \n",
      " 3   size_kwdc                      84216 non-null  float64\n",
      " 4   created_on                     84216 non-null  object \n",
      " 5   updated_on                     84148 non-null  object \n",
      " 6   production_date                84216 non-null  object \n",
      " 7   ato_date                       84216 non-null  object \n",
      " 8   actual_kwh                     80982 non-null  float64\n",
      " 9   expected_kwh                   84216 non-null  float64\n",
      " 10  weather_adjusted_expected_kwh  84216 non-null  float64\n",
      " 11  site_id                        84216 non-null  int64  \n",
      " 12  in_service_date                41423 non-null  object \n",
      " 13  host_type                      84216 non-null  object \n",
      " 14  revenue_type                   39975 non-null  object \n",
      " 15  interconnection_type           79349 non-null  object \n",
      " 16  registry_facility_name         66291 non-null  object \n",
      " 17  cref_short_name                12733 non-null  object \n",
      " 18  subscriber_orginization        12733 non-null  object \n",
      " 19  system_yield_as_built          84216 non-null  float64\n",
      " 20  system_yield_finance           84078 non-null  float64\n",
      " 21  annual_degradation_engr        82685 non-null  float64\n",
      " 22  annual_degradation_finance     32697 non-null  float64\n",
      "dtypes: float64(8), int64(3), object(12)\n",
      "memory usage: 14.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# extract column info (types, values, etc.)\n",
    "\n",
    "union.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "hour must be in 0..23",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64ns\u001b[0;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object, allow_mixed)\u001b[0m\n\u001b[1;32m   2186\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2187\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz_parsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime_to_datetime64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"K\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2188\u001b[0m             \u001b[0;31m# If tzaware, these values represent unix timestamps, so we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/_libs/tslibs/conversion.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.conversion.datetime_to_datetime64\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unrecognized value type: <class 'str'>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f98aa155728b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0munion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0munion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m    881\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtz_localize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtz\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m         \u001b[0mcache_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcache_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_maybe_cache\u001b[0;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0munique_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_dates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mcache_dates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_listlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m             \u001b[0mcache_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munique_dates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcache_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/tools/datetimes.py\u001b[0m in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, tz, unit, errors, infer_datetime_format, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0mrequire_iso8601\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequire_iso8601\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         \u001b[0mallow_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m     )\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64ns\u001b[0;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object, allow_mixed)\u001b[0m\n\u001b[1;32m   2191\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"i8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz_parsed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2192\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2193\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2195\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtz_parsed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36mobjects_to_datetime64ns\u001b[0;34m(data, dayfirst, yearfirst, utc, errors, require_iso8601, allow_object, allow_mixed)\u001b[0m\n\u001b[1;32m   2180\u001b[0m             \u001b[0myearfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myearfirst\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2181\u001b[0m             \u001b[0mrequire_iso8601\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequire_iso8601\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2182\u001b[0;31m             \u001b[0mallow_mixed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_mixed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2183\u001b[0m         )\n\u001b[1;32m   2184\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib._array_to_datetime_object\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/_libs/tslib.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslib._array_to_datetime_object\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/_libs/tslibs/parsing.pyx\u001b[0m in \u001b[0;36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(timestr, parserinfo, **kwargs)\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparserinfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDEFAULTPARSER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"String does not contain a date:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_naive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignoretz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/dateutil/parser/_parser.py\u001b[0m in \u001b[0;36m_build_naive\u001b[0;34m(self, res, default)\u001b[0m\n\u001b[1;32m   1227\u001b[0m                 \u001b[0mrepl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonthrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmonth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m         \u001b[0mnaive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweekday\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: hour must be in 0..23"
     ]
    }
   ],
   "source": [
    "# convert time-bearing columns to times (originally objects)\n",
    "\n",
    "for i in range(4, 8):\n",
    "    union[union.columns[i]] = pd.to_datetime(union[union.columns[i]])\n",
    "    \n",
    "union[union.columns[12]] = pd.to_datetime(union[union.columns[12]])\n",
    "    \n",
    "union.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union['subscriber_orginization'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Basic exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the 'holding_company_id' column, we find that contracts, total system size, and amount of data are all pretty unevenly distributed between holding companies. This finding, itself, is not surprising, but the discrepancy between the three measures may be interesting to note. \n",
    "\n",
    "For example, holding company 3 may not have many contracts, but they have a large mass of solar system which produces a lot of data. \n",
    "\n",
    "Holding companies 5, 8, and 14 have a lot of data from many smaller contracts. \n",
    "\n",
    "Holding companies 24, 30, 32, 36 have many small contracts, but not a lot of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the 'project_company_id' column, company 6 dominates in size, few large contracts with a lot of data. \n",
    "\n",
    "Projects 10 and 37 have a number of large to medium sized contracts with a lot of data.\n",
    "\n",
    "Projects 43, 44, and 56 have a lot of data with less size and less contracts.\n",
    "\n",
    "Projects 41, 48, 54, 77, and 85 have many small contracts with not very much data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot how much data we have from each contract\n",
    "\n",
    "contracts = union['contract_id'].value_counts() # count data points per contract and store it\n",
    "contracts.plot(kind = 'bar', figsize = (15, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining a simple count of contract IDs, we find many modes - 899 makes sense because data started being collected only on Jan 1, 2019 (899 days before this dataset was extracted), but others may be abnormal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts.value_counts()[contracts.value_counts() > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in contracts[contracts == 383].index:\n",
    "    print(union[union['contract_id'] == elem][['production_date', 'ato_date']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are also a number of contracts which only started collecting data on 6/1/2021, 1/1/2021, 1/1/2021, 3/1/2020, 4/1/2020, 4/1/2021, 10/1/2019, 6/1/2020, etc. (in order of frequency) regardless of their ato dates (which often don't match the first production date). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# find any discrepancies between recorded ages of each contract (from ato date to today) and no. data points\n",
    "# (should be one data point per day, but most don't match)\n",
    "\n",
    "group = union.groupby('contract_id') # group data by contract\n",
    "ato_unique = group.apply(lambda x: x['ato_date'].unique()[0]) # get a series of ato dates for each contract\n",
    "system_age = pd.to_datetime(datetime.date.today()) - ato_unique # find age by subtracting from today\n",
    "pseudo_age = pd.Series() # psuedo age bc some older contracts only started collecting data on Jan 1 2019\n",
    "for contract in contracts.index: # set any contracts w/ ato date before Jan 1 2019 to 899 days old\n",
    "    if ato_unique[contract] < pd.to_datetime(datetime.date(2019, 1, 1)):\n",
    "        pseudo_age[contract] = pd.to_datetime(datetime.date(2021, 6, 18)) - pd.to_datetime(datetime.date(2019, 1, 1))\n",
    "    else:\n",
    "        pseudo_age[contract] = system_age[contract]\n",
    "        \n",
    "differences = {} # create a dictionary to store differences between age and datapoints per contract (above)\n",
    "\n",
    "print(\"Contract \\t Age \\t Count \\t Difference\")\n",
    "for i in range(len(pseudo_age)):\n",
    "    print(f\"{pseudo_age.index[i]} \\t {pseudo_age[i].days} \\t {contracts[pseudo_age.index[i]]} \\t \\\n",
    "{contracts[pseudo_age.index[i]] - pseudo_age[i].days}\")\n",
    "    differences[pseudo_age.index[i]] = contracts[pseudo_age.index[i]] - pseudo_age[i].days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(d == 0 for d in differences.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After counting the number of data points per contract, we want to compare to the number we expect based on the sage of each solar system. We calculate this by subtracting the ato date from the date this data was delivered. As we can see above, there are many discrepancies - only 34 out of 200 contracts have the expected number of datapoints! However, most of these values are negative, meaning we have missing data (as opposed to extra/likely duplicated data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sorted(differences.values())[:-39])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We seem to be missing around 1/10 (8223/83019) of the data we expect to have. Next, we will look into extreme values (negative, too, but mostly positive), find and eliminate duplicate data.\n",
    "\n",
    "# how to deal with this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find largest discrepancies, positive and negative, for further examination\n",
    "\n",
    "print(sorted(differences.items(), key = lambda item: item[1])[:10], \"\\n\", \\\n",
    "sorted(differences.items(), key = lambda item: item[1])[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to filter dataframe for problematic contracts and examine them individually\n",
    "\n",
    "union[union['contract_id'] == 'd2Ww9Bvv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After examining some of the most extreme values, we find that, indeed, the contracts with very positive discrepancies seem to have some repeated production dates. Those with negative values are missing data. Many of the production dates start around a month or so after the ato date. Next, rows with duplicated production dates should be removed. With duplicated production dates, they seem to be mismatched on created_on and actual_kwh, with earlier (presumably non-updated) created_on dates corresponding to higher actual_kwh readings. These earlier readings will be deleted and only more recent created_on data will be maintained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete duplicated data\n",
    "duplicated = ['RZdqcB2k', 'Giw4Zk3G', 't6Pwm8dh', 'hFjQMQMM', 'd2Ww9Bvv']\n",
    "\n",
    "for contract in duplicated:\n",
    "    duplicates = union[union['contract_id'] == contract]['production_date']\n",
    "    union.drop(duplicates[duplicates.duplicated(keep = 'last')].index, inplace = True)\n",
    "    \n",
    "group = union.groupby('contract_id') # re-group data by contract since some rows were deleted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will look into the breakdown of contract type for solar system (commercial vs residential)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count unique contracts and type breakdowns for original data and supplementary data (slightly different?)\n",
    "\n",
    "id_unique = group.apply(lambda x: x['host_type'].unique()[0])\n",
    "print(id_unique.value_counts())\n",
    "print(extra['host_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one data point is residential, so we will consider it an anomaly and remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union.drop(union[union['host_type'] == 'Residential'].index, inplace = True)\n",
    "group = union.groupby('contract_id') # re-group again by contract since some rows were deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an idea of solar system sizes\n",
    "\n",
    "size_unique = group.apply(lambda x: x['size_kwdc'].unique()[0])\n",
    "plt.hist(size_unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, observe contract sizes: the histogram below is heavily skewed left w/ many small contracts and a few big ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot how many systems have been opened each month (total) - to get an idea of monthly trends\n",
    "\n",
    "month_unique = group.apply(lambda x: x['ato_date'].dt.month.unique()[0])\n",
    "plt.hist(month_unique, bins = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also plot a histogram of ato dates to see if there is a trend in the month a solar system tends to be created. As we can see, most are created in December as companies rush to meet year-end quotas, and not many are opened in January following. The peak in May is likely due to an influx of new contracts this year, May 2021, with our data for 2021 only leading up to mid-June."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot how many systems were opened each month since Sep 2016\n",
    "\n",
    "date_unique = pd.DataFrame() # new dataframe to store data by ato year and month\n",
    "date_unique['year'] = group.apply(lambda x: x['ato_date'].dt.year.unique()[0]) # columns for year\n",
    "date_unique['month'] = group.apply(lambda x: x['ato_date'].dt.month.unique()[0]) # and month\n",
    "date_unique['count'] = 1 # count to keep track\n",
    "agegroups = date_unique.groupby([date_unique['year'], date_unique['month']]).count()['count'] # count occurrences\n",
    "\n",
    "# for months with 0 occurences, still add them as indices with values/counts of 0 for the bar plot\n",
    "year = 2016\n",
    "month = 9\n",
    "while year < 2021:\n",
    "    try:\n",
    "        agegroups[(year, month)]\n",
    "        if month == 12: \n",
    "            year += 1\n",
    "            month = 1\n",
    "        else: month += 1\n",
    "    except:\n",
    "        agegroups[(year, month)] = 0\n",
    "        if month == 12: \n",
    "            year += 1\n",
    "            month = 1\n",
    "        else: month += 1\n",
    "agegroups.sort_index(inplace = True)\n",
    "\n",
    "agegroups.plot(kind = 'bar', figsize = (15, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking down our ato plot by year as well, we can see that more systems tend to be created each year as the company grows. Systems tend to peak in December, as observed before, and drop significantly for January. The fact that we only have data from September of 2016 may have also inflated the number in the month-only histogram above, causing the peak there. Interestingly, there are some months where no systems are created. Others, very few. However, evident by the peaks, the company must have the capacity to produce new systems quite quickly, making the lows potentially concerning in terms of demand. The peak this May may be due to the big deal with Franklin Park Infrastructure closed earlier this year. Below shows a cumulative plot of total systems over time (as opposed to only new systems created)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cumulative number of systems since Sep 2016\n",
    "\n",
    "agegroups.cumsum().plot(kind = 'bar', figsize = (15, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will begin looking into production, a key variable of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total production of each solar system\n",
    "\n",
    "siteproduction = {contract: 0 for contract in set(union['contract_id'])} # create a dict to store values\n",
    "\n",
    "# populate by summing production values for each contract id\n",
    "for i in union.index:\n",
    "    if np.isnan(union['actual_kwh'][i]): # skip nans\n",
    "        continue\n",
    "    siteproduction[union['contract_id'][i]] += union['actual_kwh'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg production of each system (calculated by total production of a system divided by no. data points)\n",
    "# may be inaccurate as no. data points don't always match no. production days (missing data)\n",
    "\n",
    "avgsiteproduction = {}\n",
    "# convert each contract's total production to avg by dividing by a count of datapoints per contract\n",
    "for elem in siteproduction.keys():\n",
    "    avgsiteproduction[elem] = siteproduction[elem] / union['contract_id'].value_counts()[elem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of average solar system production (skewed left w/ many smaller values and some large ones)\n",
    "\n",
    "plt.hist(avgsiteproduction.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot of average system production looks very similar to the histogram of sizes from above - this makes sense. The plot is again heavily skewed left with a few systems producing very highly and most producing smaller amounts on average. Below, plotting size against production directly, we do see that the bigger systems have higher max production values compared to smaller ones. However, the variance is also higher. The trend is that production does increase with size, as we would expect. Some smaller contracts also occasionally produce really high power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(union['size_kwdc'], union['actual_kwh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot each contract's size versus average (daily) production for comparison\n",
    "\n",
    "size_production = pd.DataFrame() # combine to a new dataframe to plot using pandas\n",
    "size_production['production'] = pd.Series(avgsiteproduction) # cast production dict as series and add column\n",
    "size_production['size'] = size_unique # size is already a series from earlier, so just add it\n",
    "size_production.sort_values(by = 'size').plot(kind = 'bar', figsize = (15, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotted individually by contract (above), again, size corresponds with average production. Below, the ratios of production/size per system are also pretty randomly distributed, if not quite uniform. However, there is one contract with net negative production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find and plot ratios of production per size\n",
    "\n",
    "size_production['ratio'] = pd.Series(avgsiteproduction)/size_unique # new column to dataframe for ratio\n",
    "size_production['ratio'].plot(kind = 'bar', figsize = (15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(size_production['ratio'].items(), key = lambda item: item[1])) # what's the one super negative value?\n",
    "print(size_production['ratio'].mean()) # and average multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the anomalous contract w/ net negative production\n",
    "\n",
    "jZWEeuhJ = union[union['contract_id'] == 'jZWEeuhJ']\n",
    "jZWEeuhJ[jZWEeuhJ['actual_kwh'] < 0] # filter for only negative production values - some EXTREMELY negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking further into the contract with negative production, we see, in fact, some VERY extreme negative values. We should probably check if there are more extreme values in the dataset and, if so, delete them so they don't skew our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all datapoints with actual production values less than -10 (concerning - for further inspection)\n",
    "\n",
    "anomalies = {}\n",
    "\n",
    "for i in union.index:\n",
    "    if data['actual_kwh'][i] < -10:\n",
    "        try:\n",
    "            anomalies[data['contract_id'][i]].append((data['production_date'][i], data['actual_kwh'][i]))\n",
    "        except:\n",
    "            anomalies[data['contract_id'][i]] = [(data['production_date'][i], data['actual_kwh'][i])]\n",
    "            \n",
    "for elem in anomalies.keys():\n",
    "    print(elem)\n",
    "    for val in anomalies[elem]:\n",
    "        print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union.drop(union[union['actual_kwh'] < -10].index, inplace = True)\n",
    "union.drop(union[union['actual_kwh'] > 100000].index, inplace = True)\n",
    "\n",
    "group = union.groupby('contract_id') # re-group for deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots below also show the clear relationship between size and production and lack thereof between either and ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(size_production['ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1423/(365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_production['production'] = size_production['production']/900\n",
    "size_production['size'] = size_production['size']/300\n",
    "size_production['ratio'] = pd.Series(avgsiteproduction)/size_unique \n",
    "\n",
    "size_production.plot(kind = 'bar', figsize = (15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_production.sort_values(by = 'ratio').plot(kind = 'bar', figsize = (15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_production.sort_values(by = 'size').plot(kind = 'bar', figsize = (15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_production.sort_values(by = 'production').plot(kind = 'bar', figsize = (15, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to check discrepancies between creation and last update to ensure our data is valid and look for potential non-comms. \n",
    "\n",
    "# wasn't the metric for 'problematic' 2 days??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, find datapoints with a lag between creation and last update (may be a sign of non-comms)\n",
    "\n",
    "discrepancies = {}\n",
    "for i in union.index:\n",
    "    discrepancies[i] = union['updated_on'][i] - union['created_on'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column in our data for discrepancies - flag any longer than 7 days (most of them???)\n",
    "\n",
    "union['discrepancy'] = discrepancies.values()\n",
    "union[union['discrepancy'] > pd.Timedelta('7 days')][['created_on', 'updated_on', 'discrepancy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union['discrepancy'].dt.days.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to plot production as a function of time. The first plot below shows total production of all systems over time (including / not factoring out the growing number of systems), the second shows average production each month since the beginning of 2019, when we started collecting data, and the final plot shows average monthly production irrespective of year. As we would expect, these graphs all fluctuate with seasons, peaking in summer with more sunlight exposure and dropping in winter with less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot total solar energy production as a function of time\n",
    "\n",
    "production_date = pd.DataFrame()\n",
    "production_date['year'] = group.apply(lambda x: x['production_date'].dt.year)\n",
    "production_date['month'] = group.apply(lambda x: x['production_date'].dt.month)\n",
    "production_date['production'] = group.apply(lambda x: x['actual_kwh'])\n",
    "productiongroups = production_date.groupby([production_date['year'], production_date['month']])\n",
    "\n",
    "productiongroups.sum()['production'].plot(kind = 'bar', figsize = (15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average energy production per contract as a function of time - cyclic with seasons\n",
    "\n",
    "productiongroups.mean()['production'].plot(kind = 'bar', figsize = (15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average production per month disregarding year\n",
    "\n",
    "productionmonth = production_date.groupby(production_date['month'])\n",
    "productionmonth.mean()['production'].plot(kind = 'bar', figsize = (15, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll also plot production by ato dates. In the initial graph, it does seem like production by systems created this year is slightly higher than those from other years, but this could be due to chance, and it's generally pretty uniform over time. In the next graph by month only, there seems to be slightly lower production during later months compared to earlier ones, but again, it doesn't look like a big difference, and the graph is generally pretty uniform, meaning no real difference in production quality between months or years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot average production of systems opened in each month since Sep 2016\n",
    "\n",
    "ato_date = pd.DataFrame()\n",
    "ato_date['year'] = group.apply(lambda x: x['ato_date'].dt.year)\n",
    "ato_date['month'] = group.apply(lambda x: x['ato_date'].dt.month)\n",
    "ato_date['production'] = group.apply(lambda x: x['actual_kwh'])\n",
    "ato_date['ratio'] = group.apply(lambda x: x['actual_kwh']/x['size_kwdc'])\n",
    "atogroups = ato_date.groupby([ato_date['year'], ato_date['month']])\n",
    "atoplot = atogroups.mean()['ratio']\n",
    "\n",
    "year = 2016\n",
    "month = 9\n",
    "while year < 2021:\n",
    "    try:\n",
    "        atoplot[(year, month)]\n",
    "        if month == 12: \n",
    "            year += 1\n",
    "            month = 1\n",
    "        else: month += 1\n",
    "    except:\n",
    "        atoplot[(year, month)] = 0\n",
    "        if month == 12: \n",
    "            year += 1\n",
    "            month = 1\n",
    "        else: month += 1\n",
    "atoplot.sort_index(inplace = True)\n",
    "\n",
    "atoplot.plot(kind = 'bar', figsize = (15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average production of systems opened each month since Sep 2016 (by ato date) disregarding year\n",
    "\n",
    "atomonth = ato_date.groupby(ato_date['month'])\n",
    "atomonth.mean()['ratio'].plot(kind = 'bar', figsize = (15, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also might be interested in production over time. It's not very practical to generate a new graph for every contract (though it may be useful to do it for specific randomly chosen / bigger contracts), so we'll just assign each contract an age and plot age against production to track potential degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column to dataframe for age\n",
    "\n",
    "union['system age'] = union.apply(lambda x: system_age[x['contract_id']].days, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot of age vs production\n",
    "\n",
    "#sns.regplot(union['age'], union['actual_kwh'])\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "degradation = union[['system age', 'actual_kwh']].dropna()\n",
    "\n",
    "# get coeffs of linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(degradation['system age'], degradation['actual_kwh'])\n",
    "\n",
    "# use line_kws to set line label for legend\n",
    "ax = sns.regplot(x = 'system age', y = 'actual_kwh', data = degradation, color='b', \\\n",
    "                 line_kws={'label':\"y={0:.1f}x+{1:.1f}\".format(slope,intercept)})\n",
    "\n",
    "# plot legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(slope) # very naive calculation of degradation in units of kwh/day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpy = slope*365 # degradation per year\n",
    "dpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppy = np.mean(union['actual_kwh'].dropna())*365 # average production per year\n",
    "ppy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dpy/ppy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union['annual_degradation_engr'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppy*0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union['ages'] = (union['production_date'] - union['in_service_date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union.groupby('contract_id').apply(lambda x: x['in_service_date']).dropna().index.get_level_values(0).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "degradations = {}\n",
    "\n",
    "for contract in contracts.index:\n",
    "    df = union[union['contract_id'] == contract][['ages', 'actual_kwh']].dropna()\n",
    "    if not df.empty:\n",
    "        degradations[contract] = stats.linregress(df['ages'], df['actual_kwh'])\n",
    "\n",
    "print(degradations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_degradation = {}\n",
    "for elem in degradations.items():\n",
    "    annual_degradation[elem[0]] = elem[1][0]/(group.mean()['actual_kwh'][elem[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(list(annual_degradation.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(annual_degradation.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union['degradation'] = union.apply(lambda row: annual_degradation[row.contract_id], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create some new columns which could be useful for modeling or perhaps further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union['production_year'] = union['production_date'].dt.year\n",
    "pyear = pd.get_dummies(union['production_year'])\n",
    "pyear.columns = [\"prod\" + str(name) for name in pyear]\n",
    "union['production_month'] = union['production_date'].dt.month\n",
    "pmonth = pd.get_dummies(union['production_month'])\n",
    "pmonth.columns = [\"prod\" + str(name) for name in pmonth]\n",
    "union['ato_year'] = union['ato_date'].dt.year\n",
    "atoyear = pd.get_dummies(union['ato_year'])\n",
    "atoyear.columns = [\"ato\" + str(name) for name in atoyear]\n",
    "union['ato_month'] = union['ato_date'].dt.month\n",
    "atomonth = pd.get_dummies(union['ato_month'])\n",
    "atomonth.columns = [\"ato\" + str(name) for name in atomonth]\n",
    "union = pd.concat([union, pyear, pmonth, atoyear, atomonth], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holding_ids = pd.get_dummies(union['holding_company_id'])\n",
    "holding_ids.columns = [\"h\" + str(name) for name in holding_ids]\n",
    "project_ids = pd.get_dummies(union['project_company_id'])\n",
    "project_ids.columns = [\"p\" + str(name) for name in project_ids]\n",
    "union = pd.concat([union, holding_ids, project_ids], axis = 1)\n",
    "union['holding_company_id'] = union['holding_company_id'].astype('category')\n",
    "union['project_company_id'] = union['project_company_id'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = union[['actual_kwh', 'expected_kwh']].copy()\n",
    "sns.regplot(subset['actual_kwh'], subset['expected_kwh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union['yield'] = union['actual_kwh']/union['size_kwdc']\n",
    "union['efficiency'] = union['actual_kwh']/union['expected_kwh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag contracts with low production\n",
    "\n",
    "union[union['efficiency'] < 0.1].groupby('contract_id').apply(lambda x: x['production_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potential loss due to weather\n",
    "\n",
    "union[union['expected_kwh'] < union['weather_adjusted_expected_kwh']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel # for modeling\n",
    "from sklearn.linear_model import Ridge, LinearRegression # also for modeling\n",
    "import statsmodels.api as statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = union.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = correlations.iloc[:10, :10].dropna(how = 'all')\n",
    "heatmap.dropna(axis = 1, how = 'all', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(heatmap, xticklabels=heatmap.columns, yticklabels=heatmap.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful = dict()\n",
    "for elem in correlations['actual_kwh'].items():\n",
    "    if abs(elem[1]) > 0.1 and abs(elem[1]) != 1:\n",
    "        useful[elem[0]] = elem[1]\n",
    "sorted(useful.items(), key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_regression(df, column_x, column_y):\n",
    "    ''' this function uses built in library functions to construct a linear \n",
    "    regression model with potentially multiple predictor variables. It outputs \n",
    "    two plots to assess the validity of the model.'''\n",
    "\n",
    "    # If there is only one predictor variable, plot the regression line\n",
    "    if len(column_x)==1:\n",
    "        plt.figure()\n",
    "        sns.regplot(x=column_x[0], y=column_y, data=df, marker=\"+\",fit_reg=True,color='orange')\n",
    "\n",
    "    # define predictors X and response Y:\n",
    "    X = df[column_x]\n",
    "    X = statsmodels.add_constant(X)\n",
    "    Y = df[column_y]\n",
    "\n",
    "    # construct model:\n",
    "    global regressionmodel \n",
    "    regressionmodel = statsmodels.OLS(Y,X).fit() # OLS = \"ordinary least squares\"\n",
    "\n",
    "    # residual plot:\n",
    "    plt.figure()\n",
    "    residualplot = sns.residplot(x=regressionmodel.predict(), y=regressionmodel.resid, color='green')\n",
    "    residualplot.set(xlabel='Fitted values for '+column_y, ylabel='Residuals')\n",
    "    residualplot.set_title('Residuals vs Fitted values',fontweight='bold',fontsize=14)\n",
    "\n",
    "    # QQ plot:\n",
    "    qqplot = statsmodels.qqplot(regressionmodel.resid,fit=True,line='45')\n",
    "    qqplot.suptitle(\"Normal Probability (\\\"QQ\\\") Plot for Residuals\",fontweight='bold',fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mult_regression(union.dropna(), list(useful.keys()), 'actual_kwh')\n",
    "regressionmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union['production_days'] = union['production_date'].astype('int64')//1e9//60//60//24 % 365\n",
    "union['ato_days'] = union['ato_date'].astype('int64')//1e9//60//60//24 % 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_regression(union.dropna(), ['size_kwdc', 'ato_days', 'annual_degradation_finance', 'production_days'], 'actual_kwh')\n",
    "regressionmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictors = dict()\n",
    "for elem in correlations['efficiency'].items():\n",
    "    if abs(elem[1]) > 0.1 and abs(elem[1]) != 1:\n",
    "        predictors[elem[0]] = elem[1]\n",
    "sorted(predictors.items(), key = lambda x: x[1])\n",
    "list(predictors.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_regression(union.dropna(), list(predictors.keys())[1:5], 'efficiency')\n",
    "regressionmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union.loc[union['efficiency'] > 2, 'efficiency'] = 2\n",
    "union.loc[union['efficiency'] < 0, 'efficiency'] = 0\n",
    "\n",
    "plt.hist(union['efficiency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(union['production_days'], union['efficiency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from kats.consts import TimeSeriesData\n",
    "\n",
    "forecast = TimeSeriesData(union[['production_date', 'actual_kwh']].dropna(), time_col_name = 'production_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the param and model classes for Prophet model\n",
    "from kats.models.prophet import ProphetModel, ProphetParams\n",
    "\n",
    "# create a model param instance\n",
    "params = ProphetParams(seasonality_mode='multiplicative') # additive mode gives worse results\n",
    "\n",
    "# create a prophet model instance\n",
    "m = ProphetModel(forecast, params)\n",
    "\n",
    "# fit model simply by calling m.fit()\n",
    "m.fit()\n",
    "\n",
    "# make prediction for next 30 month\n",
    "fcst = m.predict(steps=60, freq=\"MS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast2 = TimeSeriesData(union[['production_date', 'efficiency']], time_col_name = 'production_date')\n",
    "\n",
    "m2 = ProphetModel(forecast2, params)\n",
    "\n",
    "m2.fit()\n",
    "\n",
    "# make prediction for next 30 month\n",
    "fcst2 = m2.predict(steps=60, freq=\"MS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcst2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noncomms = union[union['actual_kwh'].isnull()][['contract_id', 'production_date']].groupby('contract_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noncommdates = noncomms.apply(lambda x: x['production_date'])\n",
    "noncommdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noncommdict = {}\n",
    "for elem in contracts.index:\n",
    "    try:\n",
    "        if len(noncommdates[elem]) > 1:\n",
    "            noncommdict[elem] = list(noncommdates[elem])\n",
    "    except:\n",
    "        pass\n",
    "sorted(noncommdict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalnoncomms = {elem: len(noncommdict[elem]) for elem in noncommdict.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(totalnoncomms.items(), key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noncommlengths = {}\n",
    "\n",
    "for elem in noncommdict.keys():\n",
    "    length = 1\n",
    "    for i in range(len(noncommdict[elem]) - 1):\n",
    "        if noncommdict[elem][i] + pd.DateOffset(1) == noncommdict[elem][i + 1]:\n",
    "            length += 1\n",
    "        else:\n",
    "            noncommlengths.append(length)\n",
    "            length = 1\n",
    "    noncommlengths.append(length)\n",
    "    \n",
    "print(noncommlengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(noncommlengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(noncommlengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noncommlengths.count(1), len(noncommlengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_of_means(data1, data2, tails):\n",
    "    n1 = len(data1)\n",
    "    n2 = len(data2)\n",
    "    n = n1 + n2\n",
    "    x1 = np.mean(data1)\n",
    "    x2 = np.mean(data2)\n",
    "    s1 = np.std(data1, ddof=1) #accounts for Bessel's correction w/ ddof=1\n",
    "    s2 = np.std(data2, ddof=1)\n",
    "    alpha = 0.05/(len(union['holding_company_id'].unique()) + len(union['project_company_id'].unique()))\n",
    "    \n",
    "    se = np.sqrt(s1**2/n1 + s2**2/n2)\n",
    "    t = (x2 - x1)/se\n",
    "    df = min(n1,n2) - 1 # conservative estimate from OpenIntro\n",
    "    pvalue = tails*stats.t.cdf(-np.abs(t), df)\n",
    "\n",
    "    SDpooled = np.sqrt((s1**2*(n1 - 1) + s2**2*(n2 - 1))/(n1 + n2 - 2)) # OpenIntro section 5.3.6\n",
    "    Cohensd = (x2 - x1)/SDpooled\n",
    "    Glassd = (x2 - x1)/s2\n",
    "    Hedgesg = Cohensd*((n - 3)/(n - 2.25))*np.sqrt((n - 2)/n) #multiplies Cohen's d by correction factor to get Hedge's g\n",
    "\n",
    "    return t, pvalue, Cohensd, Hedgesg, Glassd, pvalue < alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(f'holding id \\t t-value \\t\\t p-value \\t\\t\\t Glass\\' delta \\t\\t significant?')\n",
    "for i in union['holding_company_id'].unique():\n",
    "    company = union[union['holding_company_id'] == i]['efficiency']\n",
    "    others = union[union['holding_company_id'] != i]['efficiency']\n",
    "    if difference_of_means(company, others, 2)[1] == 0.0:\n",
    "        print(f'{i} \\t\\t {difference_of_means(company, others, 2)[0]} \\t {difference_of_means(company, others, 2)[1]} \\\n",
    "    \\t\\t\\t {difference_of_means(company, others, 2)[4]} \\t {difference_of_means(company, others, 2)[-1]}')\n",
    "    else:\n",
    "        print(f'{i} \\t\\t {difference_of_means(company, others, 2)[0]} \\t {difference_of_means(company, others, 2)[1]} \\\n",
    "    \\t {difference_of_means(company, others, 2)[4]} \\t {difference_of_means(company, others, 2)[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'project id \\t t-value \\t\\t p-value \\t\\t\\t Glass\\' delta \\t\\t significant?')\n",
    "for i in union['project_company_id'].unique():\n",
    "    company = union[union['project_company_id'] == i]['efficiency']\n",
    "    others = union[union['project_company_id'] != i]['efficiency']\n",
    "    if difference_of_means(company, others, 2)[1] == 0.0:\n",
    "        print(f'{i} \\t\\t {difference_of_means(company, others, 2)[0]} \\t {difference_of_means(company, others, 2)[1]} \\\n",
    "    \\t\\t\\t {difference_of_means(company, others, 2)[4]} \\t {difference_of_means(company, others, 2)[-1]}')\n",
    "    else:\n",
    "        print(f'{i} \\t\\t {difference_of_means(company, others, 2)[0]} \\t {difference_of_means(company, others, 2)[1]} \\\n",
    "    \\t {difference_of_means(company, others, 2)[4]} \\t {difference_of_means(company, others, 2)[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('alpha = ' + str(0.05/(len(union['holding_company_id'].unique()) + len(union['project_company_id'].unique()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(union[union['holding_company_id'] == 20]['efficiency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(union[union['holding_company_id'] == 20]['efficiency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(union[union['holding_company_id'] != 20]['efficiency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(union['efficiency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(union[union['holding_company_id'] == 20]['efficiency'], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(union[union['holding_company_id'] != 20]['efficiency'], bins = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "\n",
    "anova = ols('efficiency ~ C(holding_company_id)', data=union).fit()\n",
    "anova_table = statsmodels.stats.anova_lm(anova, typ=2)\n",
    "anova_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols\n",
    "\n",
    "anova = ols('efficiency ~ C(project_company_id)', data=union).fit()\n",
    "anova_table = statsmodels.stats.anova_lm(anova, typ=2)\n",
    "anova_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mult_regression(union.dropna(), 'size_kwdc', 'efficiency'))\n",
    "regressionmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.linregress(union.dropna()['h20'], union.dropna()['efficiency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union[union['efficiency'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = union.dropna()[holding_ids.columns]\n",
    "temp['efficiency'] = union['efficiency']\n",
    "\n",
    "X = temp[holding_ids.columns].to_numpy()\n",
    "y = temp['efficiency'].to_numpy()\n",
    "\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr.coef_\n",
    "regr.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testarray = np.array([0 for i in range(34)])\n",
    "testarray[13] = 1\n",
    "regr.predict(testarray.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x='holding_company_id', y='efficiency', data=union, color='#99c2a2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(x='project_company_id', y='efficiency', data=union, color='#99c2a2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hax = sns.boxplot(x='holding_company_id', y='actual_kwh', data=union, color='#99c2a2')\n",
    "hax = sns.swarmplot(x=\"holding_company_id\", y=\"actual_kwh\", data=union, color='#7d0013')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kats.models.sarima import SARIMAModel, SARIMAParams\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "# create SARIMA param class\n",
    "sparams = SARIMAParams(p = 2, d = 1, q = 1, trend = 'ct', seasonal_order = (1, 0, 1, 12))\n",
    "\n",
    "# initiate SARIMA model\n",
    "sm = SARIMAModel(data=forecast, params=sparams)\n",
    "\n",
    "# fit SARIMA model\n",
    "sm.fit()\n",
    "\n",
    "# generate forecast values\n",
    "sfcst = sm.predict(steps=30, freq=\"MS\")\n",
    "\n",
    "# make plot to visualize\n",
    "sm.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
